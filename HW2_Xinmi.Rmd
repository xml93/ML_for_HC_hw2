---
title: "Machine Learning for Health Care: Homework 2"
author: "Xinmi Li"
andrewID: "xinmil"
date: "February 27, 2017"
output:
  html_document:
  fig_width: 7
fig_height: 5
---

## Overview
Homework 2 is about applying what you have learned in class into analysis in R. You will draw from both your learning in lecture and discussion with the skills you are developing in the workshop sessions.

The homework is split into two parts: short questions to illustrate concepts, and a secondary analysis of data from a randomized controlled trial.

**Homework 2 is due March 6th at the beginning of class.**

### Data set
The data set used for this homework comes from the International Stroke Trial. This was a study comparing the effectiveness of medications in a populaton of patients who had suffered strokes. The publication was in the leading British medical journal Lancet:
http://www.sciencedirect.com/science/article/pii/S0140673697040117 (you may need to be on campus or use VPN)

The data set is here:
http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_corrected.csv
(more information here: http://datashare.is.ed.ac.uk/handle/10283/128)

The variable definitions files are also helpful:
http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_variables.pdf
http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_variables.csv

## Objectives
- git
- debug
- inject belief/knowledge by shifting from ML to MAP estimates
- choosing MCAR, MAR, MNAR; choosing indicator and/or imputation
- run machine learning algorithms: LR, NB, TAN, decision tree
- reporting performance, using ggplot

## Instructions

For this homework, you will use git. **To submit the homework, email me a link to your git repository.** I should be able to type "git clone <url>" and have it download from a cloud service (github, bitbucket, etc). Note that if it is a private repository, you will need to permit me access to it (please provide access to jeremy.weiss@gmail.com).

Your git repository should contain at least two commits with useful comments on what has changed from the previous version(s). This should be visible when I type in ```git log```. The submission I will grade is at the HEAD revision unless specified otherwise in your email. Include your .Rmd file and your .html file solutions in the repository with your name and andrew ID.

  
## Part 1: Concept questions (6 points)

The code that follows introduces a toy data set, decision tree model, and two prediction functions.
```{r eval=T, message=F}
library(plyr)
library(dplyr)

# synthetic depression data
depressionData = data.frame( # do not change "depressionData"
  pregnant = c(1,0,1,1),
  depressed = c("yes","yes","no","no") %>% as.factor(),
  hospitalized = c(1, 0, 0, 0) %>% as.logical()
) %>% tbl_df()

# tree: a model that outputs the odds of hospitalization from inputs of data (datums)
tree = data.frame( # do not change "tree"
  splitVariable = c("depressed", "pregnant", NA, NA, NA),
  split = c("yes", 1, NA, NA, NA),
  trueChild = c(2, 4, NA, NA, NA),
  falseChild = c(3, 5, NA, NA, NA),
  odds = c(NA, NA, 0.1, 2, 3)
)

predictOddsOnDataSet = function(tree, data, active = 1) {
  # get the predicted odds for each row in data
  apply(data, 1, (function(x) {predictedOdds(tree=tree, x, active=1)})  )
}

predictedOdds = function(tree, datum, active = 1) { # active is the node#
  
  if(is.na(tree[active,"splitVariable"])) { # leaf of tree, so output value
    
    return(tree$odds[active])
    
  } else { # internal node of tree, so continue down tree to true/false child
    
    # Categorize datum by comparing the variable value with the n split variable (n=active) in tree. With the split criterion in tree met, set the active as the true child node#, and try to get the odds OR split the child variable again... until get the odds
    if((datum[[tree[active,"splitVariable"] %>% as.character]] %>% as.character) == tree[active,"split"]) { 
      return(predictedOdds(tree, datum, active = tree[active,"trueChild"]))
    } else { 
      # with the split criterion in tree not met, set the active as the false child node#, and try to get the odds OR split the child variable again... until get the odds
      return(predictedOdds(tree, datum, active = tree[active,"falseChild"]))
    }
    
  }
  
}

# goal: run predictOddsOnDataSet(tree, depressionData)
predicted.odds <- predictOddsOnDataSet(tree, depressionData)
predicted.odds
```
  
First, verify to yourself that, for the fourth patient in ```depressionData```, the tree should have output an odds of 0.1.

Fix the function ```predictedOdds``` so that ```predictedOddsOnDataSet``` outputs the odds for each patient in data. Use the debugger functions like ```debugOnce(predictedOdds)``` or ```browser()``` to inspect the code. 

What did you change?

<font color="#157515">
Quote marks around trueChild and falseChild added, so the active will be set as the true/false child node of the current internal node.
</font>

Add a column of the predicted probabilities of hospitalization to depressionData. Display it.
```{r}
depressionData$predicted.prob <- predicted.odds/(1+predicted.odds)
depressionData
```

Using a threshold probability of 0.5, what is:

- the accuracy of the model?
- the sensitivity of the model?
- the specificity of the model?
- the precision of the model?
- the recall of the model?

```{r}
prediction <- depressionData$predicted.prob >= 0.5 
TP <- sum(as.numeric(depressionData$hospitalized == 1 & prediction == 1))
FP <- sum(as.numeric(depressionData$hospitalized == 0 & prediction == 1))
TN <- sum(as.numeric(depressionData$hospitalized == 0 & prediction == 0))
FN <- sum(as.numeric(depressionData$hospitalized == 1 & prediction == 0))
```

<font color="#157515">
The accuracy is `r (TP+TN)/4`.
The sensitivity is `r TP/(TP+FN)`.
The specificity is `r TN/(TN+FP)`.
The precision is `r TP/(TP+FP)`.
The recall is `r TP/(TP+FN)`.
</font>

Suppose you want to know the prevalence of diabetes in Pittsburgh. If you randomly survey 10 Pittsburghers and 5 of them state they have diabetes:

- what is the maximum likelihood estimate for the prevalence of diabetes?
```{r}
p <- seq(0,1,0.001)
MLE <- p[which.max(dbeta(p, shape1 = 5, shape2 = 5))]
MLE
```

<font color="#157515">
The maximum likelihood estimate is 0.5.
</font>

- given your strong belief specified by a beta prior of $\alpha = 11, \beta = 21$, what is the maximum a posteriori estimate for the prevalence of diabetes?

```{r}
MLE.new <- p[which.max(dbeta(p, shape1 = 5+11, shape2 = 5+21))]
MLE.new
```

<font color="#157515">
With prior belief, the maximum posteriori estimate is 0.375.
</font>

## Part 2: Analysis (9 points)

#### Preliminaries
- **Y:** What was the definition of the primary outcome in this study?
- What is (are) the variable name(s) for the outcome?

- **U:** what is (are) the variable name(s) for the intervention, and what is (are) their possible values?

- **V, W:** describe the covariates included and the population being studied.

<font color="#157515">
**Y:**

The primary outcomes are: (a) the incidence of death from any cause within 14 days, with the variable name "ID14", and (b) the incidence of death or dependency at 6 months, with the variable name "FDEAD" and "FDENNIS".

**U:**

The variable names for intervention are: "RXASP" with possible values {Y, N} and "RXHEP" with possible values {M, L, N}.

**V, W:** 

The covariates are delay in hours from symptoms, age, sex, onset conditions, conscious level, cardiac rhythm, systolic BP, stroke syndrom, leg weakness, CT scan, appearance of pre-randomisation CT, and pre-randomisation antithrombotic therapy.

The studied population are 19435 patients who are eligible if: (a) with evidence of an acute stroke with onset less than 48 hours previously, (b) with no evidence of intracranial haemorrhage, and (c) with no clear indications for orcontradications to heparin or aspirin. The fundamental criterion is physician's uncertaining whether or not to assign either or both of the trial treatments to the particular patient. 
The exclusions are: only a small likelihood of worthwhile benefit or a high risk of adverse effects on the patiens.
</font>


- Construct a so-called Table 1 for groups of {aspirin, no aspirin} use, including information on age, gender, systolic blood pressure, and conscious state.

```{r}
IST <- data.frame(read.csv(file = "/Users/Xinmi/git/ML_for_HC_hw2/IST.csv", header = TRUE))
Table1 <- IST %>%
  subset(select = c("RXASP", "AGE", "SEX", "RSBP","RCONSC")) %>%
  transform(RXASP = as.numeric(RXASP == "Y"))
colnames(Table1) <- (c("Aspirin", "Age", "Sex", "SystolicBP", "Consicousness"))
head(Table1)
```


#### Machine learning analysis
Note: for this analysis, use a simple 50-50 train-test split.

Let our outcome of interest be "dead or dependent at 6 months", i.e. so that we have a binary classification problem. What percent of patients are dead or dependent at 6 months in your train set and test set?
```{r}
ordering <- sample(1:nrow(IST))
train <- IST[ordering[1:50],]
test <- IST[ordering[(length(ordering)-50+1):length(ordering)], ]
```

<font color="#157515">
The percent of patients dead or dependent at 6 months is `r round((sum(as.numeric(train$FDEAD=="Y")) + sum(as.numeric(train$FDENNIS=="Y")))/nrow(train) * 100, 2)`% in the train set and `r round((sum(as.numeric(test$FDEAD=="Y")) + sum(as.numeric(test$FDENNIS=="Y")))/nrow(test) * 100, 2)`% in the test set.
</font>

Choose which variables to include in your model. For example, remove variables for outcomes at 14 days (because if you are dead at 14 days you are certainly dead at 6 months). Moreover, you should remove all features measured after baseline if you want to make a prediction based on baseline data. Similarly, specific indicators of the outcome should also be removed, since those are measurements past the baseline that are not our outcome of interest. For these reasons, you will need to remove clusters of variables. Justify your approach.
```{r}
y <- c("FDEAD", "FDENNIS")
v <- c("RDELAY", "RCONSC", "SEX", "AGE", "RSLEEP", "RATRIAL", "RCT", "RVISINF", "RHEP24", "RASP3", "RSBP", "STYPE")
u <- c("RXASP", "RXHEP")

train <- train %>%
  subset(select = c(y, v, u)) %>%
  transform(Outcome = as.numeric(FDEAD == "Y" | FDENNIS == "Y")) %>%
  subset(select = -c(FDEAD, FDENNIS))

test <- test %>%
  subset(select = c(y, v, u)) %>%
  transform(Outcome = as.numeric(FDEAD == "Y" | FDENNIS == "Y")) %>%
  subset(select = -c(FDEAD, FDENNIS))
```

<font color="#157515">
Only outcomes, covariates, and intervention variables are kept in the train and test sets. The outcome is transformed into a binary value that uses 1 for "death or dependency at 6 months". 
</font>


Of the remaining variables, decide whether to exclude variables with missing data, impute them, and/or use indicator variables. (Note that if you choose multiple imputation for some variables, you would need to pool the results when evaluating performance, however for homework you may just use the first imputed data set). Justify your approach.
```{r}
library(mice)
library(lattice)

train <- train %>%
  apply(2, function(x) {mapvalues(x, "", NA)}) %>%
  as.data.frame()

test <- test %>%
  apply(2, function(x) {mapvalues(x, "", NA)}) %>%
  as.data.frame()

md.pattern(train)
md.pattern(test)

train.imp <- mice(data = train, m = 5, maxit = 20, seed = 0)

train.complete <- complete(train.imp,1)
```
<font color="#157515">
Since the missing percentage of each column is small, the missing values are very likely to miss at random. All variables should be included. 
</font>



Use the following machine learning algorithms: logistic regression, naive Bayes, Tree Augmented Naive Bayes, and decision tree (specify any parameters you set that are not the default). The packages that you may find useful here are: "glm", "bnlearn", and "rpart", but you may use others if desired. In a table, report the accuracy with 95% confidence intervals for each algorithm.

Logistic Regression
```{r}
train.lr <- train.complete %>%
  transform(RDELAY = as.numeric(RDELAY), AGE = as.numeric(AGE), RSBP = as.numeric(RSBP))%>%  
  subset(select = apply(train.complete, 2, function(x) {length(unique(x))}>1))

lr <- glm(Outcome == 1 ~ ., family=binomial(link="logit"), data = train.lr)
summary(lr)

# Change the factor level in test set unseen in train set into NA.
test.lr <- test %>%
  transform(RDELAY = as.numeric(RDELAY),
            AGE = as.numeric(AGE),
            RSBP = as.numeric(RSBP),
            STYPE = mapvalues(STYPE, "OTH", NA))

summary.lr <- as.vector(predict(lr, test.lr)>=0.5) %>%
  table(test.lr$Outcome)
summary.lr

accuracy.lr <- (summary.lr[1,1]+summary.lr[2,2])/sum(colSums(summary.lr))
error.lr <- 1 - accuracy.lr
CI.lr.lower <- error.lr - 1.96 * sqrt(error.lr*accuracy.lr/sum(colSums(summary.lr)))
CI.lr.upper <- error.lr + 1.96 * sqrt(error.lr*accuracy.lr/sum(colSums(summary.lr)))

lr.report <- data.frame(accuracy.lr, CI.lr.lower, CI.lr.upper)
lr.report
```

Construct an ROC (receiver operating characteristic) curve for each model and overlay them on a graph using ggplot. Include a legend.
[response required]

Construct a PR (precision recall) curve for each model. Include a legend.
[response required]

#### Conclusions
Let's draw conclusions from this study. Specifically,

- how well are we able to predict death or dependence at 6 months? 
[response required]

- what is the average treatment effect of aspirin on death or dependence at 6 months? Is aspirin significantly better than the alternative? 
[response required]

- of the algorithms tested, which algorithms perform the best? Justify your statement.
[response required]

Congratulations, you've conducted a comparison of machine learning algorithms for mortality prediction! Commit your solutions to your git repository with an informative comment. ```git push``` will help you upload it to the cloud service you choose to use (github, bitbucket, etc).